{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed58c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "# Third-party imports\n",
    "import spacy\n",
    "import trafilatura\n",
    "import ibm_db\n",
    "import ibm_db_dbi\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "# LangChain core imports\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import convert_to_messages\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# LangChain community imports\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "# LangChain IBM imports\n",
    "from langchain_ibm import ChatWatsonx\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams, EmbedTextParamsMetaNames\n",
    "\n",
    "# LangChain DB2 imports\n",
    "from langchain_db2 import db2vs\n",
    "from langchain_db2.db2vs import DB2VS\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Load environment variables with explicit path and override\n",
    "load_dotenv(os.path.join(os.getcwd(), \".env\"), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696a1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "DB_NAME = os.getenv(\"DB_NAME\", \"\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"\")\n",
    "DB_PROTOCOL = os.getenv(\"DB_PROTOCOL\", \"\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"\")\n",
    "LLM_PATH = os.getenv(\"LLM_PATH\", \"\")\n",
    "EMBEDDING_MODEL_PATH = os.getenv(\"EMBEDDING_MODEL_PATH\", \"\")\n",
    "\n",
    "\n",
    "conn_str=f\"DATABASE={DB_NAME};hostname={DB_HOST};port={DB_PORT};protocol={DB_PROTOCOL};uid={DB_USER};pwd={DB_PASSWORD}\"\n",
    "\n",
    "try:\n",
    "    connection = ibm_db_dbi.connect(conn_str, '', '')\n",
    "    print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(\"Connection failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa337e93",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c068ef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despite being one of the earlier machine learning techniques, linear regression continues to be a top choice among ML practitioners for a regression task. For the past three years, over 80% of the respondents to Kaggleâ€™s annual state of data science and machine learning survey mentioned linear regression as a ML algorithm they most frequently use. IBM Db2 provides an in-database stored procedure (SP) for Linear Regression as part of its ML library, which is a collection of over 200 SPs for performing different ML tasks in the database. Using the linear regression SP and other functionality of DB2â€™s ML Library, ML practitioners can build and deploy linear regression models in the database when their ML dataset is available in a Db2 database. In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\n",
      "Letâ€™s begin.\n",
      "The Regression Task\n",
      "In this exercise, I will use the GoSales dataset, which is available from this link.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://community.ibm.com/community/user/blogs/shaikh-quader/2024/05/07/building-an-in-db-linear-regression-model-with-ibm'\n",
    "downloaded = trafilatura.fetch_url(url)\n",
    "\n",
    "if downloaded:\n",
    "    article = trafilatura.extract(downloaded)\n",
    "    print(article[:1000])  # Preview first 1000 chars\n",
    "else:\n",
    "    print(\"Failed to fetch content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a2b1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea57bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_sentence_chunker(text, max_words=200, overlap_words=50):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        sentence = sentences[i]\n",
    "        sentence_length = len(sentence.split())\n",
    "\n",
    "        if current_length + sentence_length <= max_words:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "            i += 1\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            # Start new chunk with overlap\n",
    "            overlap = []\n",
    "            overlap_len = 0\n",
    "            j = len(current_chunk) - 1\n",
    "            while j >= 0 and overlap_len < overlap_words:\n",
    "                s = current_chunk[j]\n",
    "                overlap.insert(0, s)\n",
    "                overlap_len += len(s.split())\n",
    "                j -= 1\n",
    "            current_chunk = overlap\n",
    "            current_length = overlap_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd8db76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 chunks created.\n",
      "Despite being one of the earlier machine learning techniques, linear regression continues to be a top choice among ML practitioners for a regression task. For the past three years, over 80% of the respondents to Kaggleâ€™s annual state of data science and machine learning survey mentioned linear regression as a ML algorithm they most frequently use. IBM Db2 provides an in-database stored procedure (SP) for Linear Regression as part of its ML library, which is a collection of over 200 SPs for performing different ML tasks in the database. Using the linear regression SP and other functionality of DB2â€™s ML Library, ML practitioners can build and deploy linear regression models in the database when their ML dataset is available in a Db2 database. In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\n",
      "Letâ€™s begin. The Regression Task\n",
      "In this exercise, I will use the GoSales dataset, which is available from this link. The dataset has 60252 synthetic customersâ€™ profile and their purchase amount at an imaginary outdoor equipment store.\n"
     ]
    }
   ],
   "source": [
    "chunks = overlapping_sentence_chunker(article, max_words=200, overlap_words=50)\n",
    "print(f\"{len(chunks)} chunks created.\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97296a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Despite being one of the earlier machine learning techniques, linear regression continues to be a top choice among ML practitioners for a regression task. For the past three years, over 80% of the respondents to Kaggleâ€™s annual state of data science and machine learning survey mentioned linear regression as a ML algorithm they most frequently use. IBM Db2 provides an in-database stored procedure (SP) for Linear Regression as part of its ML library, which is a collection of over 200 SPs for performing different ML tasks in the database. Using the linear regression SP and other functionality of DB2â€™s ML Library, ML practitioners can build and deploy linear regression models in the database when their ML dataset is available in a Db2 database. In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\\nLetâ€™s begin. The Regression Task\\nIn this exercise, I will use the GoSales dataset, which is available from this link. The dataset has 60252 synthetic customersâ€™ profile and their purchase amount at an imaginary outdoor equipment store.',\n",
       " 'In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\\nLetâ€™s begin. The Regression Task\\nIn this exercise, I will use the GoSales dataset, which is available from this link. The dataset has 60252 synthetic customersâ€™ profile and their purchase amount at an imaginary outdoor equipment store. The following is the list of columns in the dataset:\\nI want to learn a multiple linear regression function of the following equation form that will use as input the first four columns â€” AGE, MARITAL_STATUS, and PROFESSION â€” and predict the PURCHASE_AMOUNT. Using the training examples, the linear regression algorithm will learn the values of the following five parameters â€” the first one is the intercept and the remaining are four coefficients, one per input column. Following these steps, I created a Db2 table with the name GOSALES_FULL under GOSALES schema and loaded the dataset into it. Train / Test Split\\nFirst, I will divide the records from the GOSALES_FULL table into two partitions: a training partition and a test partition. For making these partitions, I will call the SPLIT_DATA stored procedure (SP) as follows.',\n",
       " \"Following these steps, I created a Db2 table with the name GOSALES_FULL under GOSALES schema and loaded the dataset into it. Train / Test Split\\nFirst, I will divide the records from the GOSALES_FULL table into two partitions: a training partition and a test partition. For making these partitions, I will call the SPLIT_DATA stored procedure (SP) as follows. CALL IDAX.SPLIT_DATA('intable=GOSALES.GOSALES_FULL, id=ID, traintable=GOSALES.GOSALES_TRAIN, testtable=GOSALES.GOSALES_TEST, fraction=0.8, seed=1') These are the parameters I passed to this SP: (1) intable: the input table from which I want to create partitions. The value is GOSALES_FULL; (2) id: the name of the id column in the input table; (3) traintable: the name I want to give to the table that will have the training records; (4) testtable: the name of the output table where the SP will store the test records; (5) fraction: the portion of the records that I want in the training partition; (6) seed: I can set a value for reproducibility of the same partitions. The SP will randomly divide records from the GOSALES_FULL table into two output tables: GOSALES_TRAIN and GOSALES_TEST. The following two SQL statements will count the number of records in these two tables.\",\n",
       " \"The value is GOSALES_FULL; (2) id: the name of the id column in the input table; (3) traintable: the name I want to give to the table that will have the training records; (4) testtable: the name of the output table where the SP will store the test records; (5) fraction: the portion of the records that I want in the training partition; (6) seed: I can set a value for reproducibility of the same partitions. The SP will randomly divide records from the GOSALES_FULL table into two output tables: GOSALES_TRAIN and GOSALES_TEST. The following two SQL statements will count the number of records in these two tables. SELECT count(*) FROM GOSALES.GOSALES_TRAIN\\nSELECT count(*) FROM GOSALES.GOSALES_TEST\\nThe above counts confirm that the train and test tables have 80% and 20% of records, respectively, from the original table with 60252 records. Data Exploration\\nNow, I will look into some sample records from the training dataset, GOSALES_TRAIN. SELECT * FROM GOSALES.GOSALES_TRAIN FETCH FIRST 5 ROWS ONLY\\nFrom the above sample records, I get a feel for the customer records I will work with. Next, I will generate summary statistics of the entire training set using SUMMARY1000 SP:\\nCALL IDAX.SUMMARY1000('intable=GOSALES.GOSALES_TRAIN, outtable=GOSALES.GOSALES_TRAIN_SUM1000, incolumn=GENDER;AGE;MARITAL_STATUS;PROFESSION')\",\n",
       " \"Data Exploration\\nNow, I will look into some sample records from the training dataset, GOSALES_TRAIN. SELECT * FROM GOSALES.GOSALES_TRAIN FETCH FIRST 5 ROWS ONLY\\nFrom the above sample records, I get a feel for the customer records I will work with. Next, I will generate summary statistics of the entire training set using SUMMARY1000 SP:\\nCALL IDAX.SUMMARY1000('intable=GOSALES.GOSALES_TRAIN, outtable=GOSALES.GOSALES_TRAIN_SUM1000, incolumn=GENDER;AGE;MARITAL_STATUS;PROFESSION') In this SP call, intable parameter has the name of the training table from which I wanted to gather summary statistics. outtable parameter has the name of the output table where I want the SP to save the summary statistics of the entire training table. In the incolumn parameter, I list the columns whose statistics I want to collect. I am calling this SP with these parameters: (1) intable: name of the table whose sumary statistics I want to gather, the training partition in this case; (2) outtable: name of the table where I want the SP to store the overall summary statistics, (3) incolumn: list of columns whose statistics I want to collect.\",\n",
       " 'I am calling this SP with these parameters: (1) intable: name of the table whose sumary statistics I want to gather, the training partition in this case; (2) outtable: name of the table where I want the SP to store the overall summary statistics, (3) incolumn: list of columns whose statistics I want to collect. SUMMARY1000 SP saves the collected statistics from the training table in three output tables: (1) GOSALES_TRAIN_SUM1000: has summary statistics of all columns listed in the incolumn parameter; (2) GOSALES_TRAIN_SUM1000_NUM: has summary statistics of only the numeric columns from the incolumn parameter, (3) GOSALES_TRAIN_SUM1000_CHAR: has summary statistics of categorical columns from the incolumn parameter. For the ease of viewing, I will look at the summary statistics of each column type separately â€” first the numeric type and then nominal type. SELECT * FROM GOSALES.GOSALES_TRAIN_SUM1000_NUM The dataset has one numeric feature column, AGE. This summary table has provides a range of statistics, such as mean, variance, skewness. Also, the table reports that the AGE column has 1878 missing values. Similarly, I find the following summary statistics from the GOSALES.GOSALES_TRAIN_SUM1000_CHAR table.',\n",
       " \"For the ease of viewing, I will look at the summary statistics of each column type separately â€” first the numeric type and then nominal type. SELECT * FROM GOSALES.GOSALES_TRAIN_SUM1000_NUM The dataset has one numeric feature column, AGE. This summary table has provides a range of statistics, such as mean, variance, skewness. Also, the table reports that the AGE column has 1878 missing values. Similarly, I find the following summary statistics from the GOSALES.GOSALES_TRAIN_SUM1000_CHAR table. SELECT * FROM GOSALES.GOSALES_TRAIN_SUM1000_CHAR\\nFrom this summary, I can see that all three nominal columns have some missing values. Data Preprocessing\\nFrom data exploration, I identified two kinds of data preprocessing tasks: handling missing values and dealing with nominal columns. In this step, I will address both tasks. Dealing with missing values\\nI will replace missing values in the training dataset using IMPUTE_DATA SP. This SP supports replacing missing values with one of the four methods: mean, median, most frequent value, or a constant. All four methods work with numeric columns, whereas only the last two methods apply to nominal columns. AGE column: I will replace missing values in the AGE column with the mean age value:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, incolumn=AGE, method=mean')\",\n",
       " \"This SP supports replacing missing values with one of the four methods: mean, median, most frequent value, or a constant. All four methods work with numeric columns, whereas only the last two methods apply to nominal columns. AGE column: I will replace missing values in the AGE column with the mean age value:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, incolumn=AGE, method=mean') In the above SP call, I passed three parameters: (1) intable: the name of the input table, which is GOSALES_TRAIN, (2) incolumn: the name of the column for imputing missing values. AGE is the column name. (3) method: a supported imputation strategy. I chose mean.) Similarly, the following SP calls will replace missing values in GENDER, MARITAL_STATUS, and PROFESSION columns with the most frequent value of each column. I looked up the most frequent values of each column from the GOSALES_TRAIN_SUM1000_CHAR table that was created during the data exploration step. Imputing missing values in the GENDER column:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, method=replace, nominalValue=M, incolumn=GENDER') Imputing missing values in the MARITAL_STATUS column:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, method=replace, nominalValue=Married, incolumn=MARITAL_STATUS')\",\n",
       " \"Similarly, the following SP calls will replace missing values in GENDER, MARITAL_STATUS, and PROFESSION columns with the most frequent value of each column. I looked up the most frequent values of each column from the GOSALES_TRAIN_SUM1000_CHAR table that was created during the data exploration step. Imputing missing values in the GENDER column:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, method=replace, nominalValue=M, incolumn=GENDER') Imputing missing values in the MARITAL_STATUS column:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, method=replace, nominalValue=Married, incolumn=MARITAL_STATUS') Imputing missing values in the PROFESSION column:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, method=replace, nominalValue=Other, incolumn=PROFESSION')\\nBy running the following SQL statement, I can confirm that the AGE column in the training dataset is now free of any missing values:\\nSELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE AGE IS NULL\\nSimilarly, the following statements will count missing values in GENDER, MARITAL_STATUS, and PROFESSION columns. All of them no longer have any missing value. Count missing values in the GENDER COLUMN:\\nSELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE GENDER IS NULL\\nCount missing values in the MARITAL_STATUS column:\\nSELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE MARITAL_STATUS IS NULL\\nCount missing values in the PROFESSION column:\\nSELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE PROFESSION IS NULL\\nDealing with nominal columns\\nLinear regression algorithm requires that all its input columns are numeric.\",\n",
       " 'Count missing values in the GENDER COLUMN:\\nSELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE GENDER IS NULL\\nCount missing values in the MARITAL_STATUS column:\\nSELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE MARITAL_STATUS IS NULL\\nCount missing values in the PROFESSION column:\\nSELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE PROFESSION IS NULL\\nDealing with nominal columns\\nLinear regression algorithm requires that all its input columns are numeric. So, before invoking a linear regression algorithm, ML practitioners convert any non-numeric columns to numbers following some encoding scheme â€” such as 1-hot encoding. The linear regression SP of Db2 can natively handle nominal columns. When the input table has any nominal column, the SP internally converts it into a set of numeric columns using 1-hot encoding. So, I can leave the three nominal columns â€” GENDER, MARITAL_STATUS, and PROFESSION â€” as-is and let the SP take care of them. Now, the GOSALES_TRAIN dataset is ready for model training. Model Training\\nThe following call to the LINEAR_REGRESSION SP will begin training of a linear regression model using training examples from the GOSALES_TRAIN table. The SP will use the list of columns mentioned in the incolum paramter as input features and the column from the target parameter as the output column.',\n",
       " \"Model Training\\nThe following call to the LINEAR_REGRESSION SP will begin training of a linear regression model using training examples from the GOSALES_TRAIN table. The SP will use the list of columns mentioned in the incolum paramter as input features and the column from the target parameter as the output column. Since the intercept parameter is set to true, the SP will learn the value of intercept. CALL IDAX.LINEAR_REGRESSION('model=GOSALES.GOSALES_LINREG, intable=GOSALES.GOSALES_TRAIN, id=ID, target=PURCHASE_AMOUNT,incolumn=AGE;GENDER;MARITAL_STATUS;PROFESSION, intercept=true') After the training completes, the SP will add the new model, GOSALES_LINREG, to Db2â€™s model catalog. The following SP call will show the list of existing models in the catalog, which now includes my new model. CALL IDAX.LIST_MODELS('format=short, all=true')\\nAdditionally, the LINEAR_REGRESSION SP saves the learned values of intercept and the coefficients, along with several other learned parameter values, in a table. The tableâ€™s name takes this form: MODELNAME_MODEL. For the GOSALES_LINREG model, the name of its metadata table is GOSALES_LINREG_MODEL. The following SQL will display the values of the learned model parameters. SELECT VAR_NAME, LEVEL_NAME, VALUE FROM GOSALES.GOSALES_LINREG_MODEL You may have noticed that the above output has more feature columns than what I originally had in the training set.\",\n",
       " \"For the GOSALES_LINREG model, the name of its metadata table is GOSALES_LINREG_MODEL. The following SQL will display the values of the learned model parameters. SELECT VAR_NAME, LEVEL_NAME, VALUE FROM GOSALES.GOSALES_LINREG_MODEL You may have noticed that the above output has more feature columns than what I originally had in the training set. Because LINEAR_REGRESSION SP has applied 1-hot encoding on the nominal columns, the final feature set has one feature for each distinct value in the nominal columns. Generating Predictions with the Model In this step, I will use the GOSALES_LINREG model to predict the purchase amount of the customers in the GOSALE_TEST table. Before generating predictions, I will preprocess this dataset using the preprocessing steps I had used with the training dataset, which were imputing missing values. I will use the following SQL statements to impute missing values in AGE, GENDER, MARITAL_STATUS, and PROFESSION columns of the test dataset. Imputing missing values in the AGE column:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=mean, incolumn=AGE') Imputing missing values in the GENDER column:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=replace, nominalValue=M, incolumn=GENDER')\\nImputing missing values in the MARITAL_STATUS column:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=replace, nominalValue=Married, incolumn=MARITAL_STATUS')\",\n",
       " \"I will use the following SQL statements to impute missing values in AGE, GENDER, MARITAL_STATUS, and PROFESSION columns of the test dataset. Imputing missing values in the AGE column:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=mean, incolumn=AGE') Imputing missing values in the GENDER column:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=replace, nominalValue=M, incolumn=GENDER')\\nImputing missing values in the MARITAL_STATUS column:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=replace, nominalValue=Married, incolumn=MARITAL_STATUS') Imputing missing values in the PROFESSION column:\\nCALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=replace, nominalValue=Other, incolumn=PROFESSION')\\nNow the test dataset is ready to be passed as input to the model. PREDICT_LINEAR_REGRESSION SP will use GOSALES_LINREG model to generate predictions for records in the GOSALES_TEST table. The SP will save the predictions in the GOSALES_TEST_PREDICTIONS table, as per the table name specified in the outtable parameter. CALL IDAX.PREDICT_LINEAR_REGRESSION('model=GOSALES.GOSALES_LINREG, intable=GOSALES.GOSALES_TEST, outtable=GOSALES.GOSALES_TEST_PREDICTIONS, id=ID') The following SQL displays sample predictions from the GOSALES_TEST_PREDICTIONS table. SELECT * FROM GOSALES.GOSALES_TEST_PREDICTIONS FETCH FIRST 5 ROWS ONLY\\nModel Evaluation\\nFor the test customer records, the actual purchase price is available at the GOSALES_TEST table and the predicted purchase price is in the GOSALES_TEST_PREDICTIONS table. Based on these actual and the predicted purchase amounts, I can now compute Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Absolute Percent Error (MAPE) via SQL.\",\n",
       " \"SELECT * FROM GOSALES.GOSALES_TEST_PREDICTIONS FETCH FIRST 5 ROWS ONLY\\nModel Evaluation\\nFor the test customer records, the actual purchase price is available at the GOSALES_TEST table and the predicted purchase price is in the GOSALES_TEST_PREDICTIONS table. Based on these actual and the predicted purchase amounts, I can now compute Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Absolute Percent Error (MAPE) via SQL. This will help me evaluate the modelâ€™s predictive performance. MSE:\\nI will use MSE SP to compute the MSE:\\nCALL IDAX.MSE('intable=GOSALES.GOSALES_TEST, id=ID, target=PURCHASE_AMOUNT, resulttable=GOSALES.GOSALES_TEST_PREDICTIONS, resulttarget=PURCHASE_AMOUNT')\\nMAE:\\nI will use MAE SP to calculate MAE:\\nCALL IDAX.MAE('intable=GOSALES.GOSALES_TEST, id=ID, target=PURCHASE_AMOUNT, resulttable=GOSALES.GOSALES_TEST_PREDICTIONS, resulttarget=PURCHASE_AMOUNT')\\nMAPE: I wrote the following SQL to compute MAPE: SELECT avg(abs(A.PURCHASE_AMOUNT - B.PURCHASE_AMOUNT) / A.PURCHASE_AMOUNT * 100) AS MAPE FROM GOSALES.GOSALES_TEST AS A, GOSALES.GOSALES_TEST_PREDICTIONS AS B WHERE A.ID = B.ID\\nDropping the Model\\nIf I want to drop this model, I can use the DROP_MODEL SP:\\nCALL IDAX.DROP_MODEL('model=GOSALES.GOSALES_LINREG')\\nConclusion\\nIn this exercise, I built, evaluated, and deployed an end-to-end linear regression pipeline using 29 simple SQL queries â€” 17 of them are calls to SPs provided by Db2, and the remaining ones were SELECT statements.\",\n",
       " \"A.PURCHASE_AMOUNT * 100) AS MAPE FROM GOSALES.GOSALES_TEST AS A, GOSALES.GOSALES_TEST_PREDICTIONS AS B WHERE A.ID = B.ID\\nDropping the Model\\nIf I want to drop this model, I can use the DROP_MODEL SP:\\nCALL IDAX.DROP_MODEL('model=GOSALES.GOSALES_LINREG')\\nConclusion\\nIn this exercise, I built, evaluated, and deployed an end-to-end linear regression pipeline using 29 simple SQL queries â€” 17 of them are calls to SPs provided by Db2, and the remaining ones were SELECT statements. In this ML workflow, I didnâ€™t need to bring any data outside of the database, nor did I need separate infrastructure for developing, training, and serving ML models. For many companies, machine learning in the database can be a cost effective and quicker path to embrace AI.\\nRelated Resources\\nFor creating an in-database ML pipeline using my above steps, you can download the GoSales dataset from this link and follow instructions on this page to set up a Db2 database. To learn more about Db2â€™s in-database ML library, check out Db2 production documentation. To find more examples of creating in-database ML models with Db2, check out this GitHub repo. #Featured-area-2\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa1e1f",
   "metadata": {},
   "source": [
    "## 2. Create a retriever tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a36259",
   "metadata": {},
   "source": [
    "Set up embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7acfb75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture init_logs\n",
    "\n",
    "# embeddings = LlamaCppEmbeddings(model_path=EMBEDDING_MODEL_PATH)\n",
    "embeddings = LlamaCppEmbeddings(\n",
    "    model_path=EMBEDDING_MODEL_PATH,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91388c77",
   "metadata": {},
   "source": [
    "Set up vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1399732c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
     ]
    }
   ],
   "source": [
    "vectorstore = DB2VS.from_texts(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    client=connection,\n",
    "    table_name=\"Documents_EUCLIDEAN\",\n",
    "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a17db19",
   "metadata": {},
   "source": [
    "# Set up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c4f66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture init_logs\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=LLM_PATH,\n",
    "    n_gpu_layers=0,\n",
    "    n_threads=30,\n",
    "    n_batch=512,\n",
    "    max_tokens=250,       \n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    temperature=0.3,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    repeat_penalty=1.1,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1cc29",
   "metadata": {},
   "source": [
    "# The RAG Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f83046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better prompt that encourages combining information\n",
    "template = \"\"\"<|system|>\n",
    "You are a helpful assistant. When answering questions, synthesize information from all the provided context to give a comprehensive, well-structured response.<|end|>\n",
    "<|user|>\n",
    "Context Information:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions: \n",
    "- Use ALL relevant information from the context above\n",
    "- Combine related points into a coherent answer\n",
    "- Organize your response clearly with examples when available\n",
    "- If multiple sources mention the same topic, integrate them smoothly<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fdb0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify your RAG to return source documents\n",
    "rag = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    verbose=False,\n",
    "    return_source_documents=True  # This returns the retrieved context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c04ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## ðŸ’¡ Answer\n",
       "\n",
       " To build a linear regression model in Db2, follow these steps:\n",
       "\n",
       "1. **Prepare the Dataset**: Ensure that your dataset is clean and ready for analysis. In this case, we have already loaded the GoSales dataset into a Db2 table named GOSALES_FULL.\n",
       "\n",
       "2. **Split the Data**: Divide the records from the GOSALES_FULL table into two partitions: a training partition and a test partition. This process ensures that the model is trained on data not used for testing, which helps in evaluating the performance of the model more accurately. The following SQL query can be used to split the data:\n",
       "\n",
       "```sql\n",
       "CALL IDAX.SPLIT_DATA('GOSALES_FULL', 'GOSALES.LINREG_TRAIN', 'GOSALES.LINREG_TEST');\n",
       "```\n",
       "\n",
       "3. **Train the Linear Regression Model**: Use the training partition of the dataset to train a linear regression model. The following SQL query can be used to create and train the linear regression model:\n",
       "\n",
       "```sql\n",
       "CALL IDAX.CREATE_MODEL('model=GOSALES.GOSALES_LINREG', 'algorithm=linear_regression', 'target_column=MAPE', 'training_data_partition=GOSALES.LINREG_TRAIN');\n",
       "```\n",
       "\n",
       "4. **Evaluate the Model\n",
       "\n",
       "---\n",
       "\n",
       "## ðŸ“š Retrieved Context\n",
       "\n",
       "\n",
       "**ðŸ“„ Chunk 1**\n",
       "\n",
       "Despite being one of the earlier machine learning techniques, linear regression continues to be a top choice among ML practitioners for a regression task. For the past three years, over 80% of the respondents to Kaggleâ€™s annual state of data science and machine learning survey mentioned linear regression as a ML algorithm they most frequently use. IBM Db2 provides an in-database stored procedure (SP) for Linear Regression as part of its ML library, which is a collection of over 200 SPs for performing different ML tasks in the database. Using the linear regression SP and other functionality of DB2â€™s ML Library, ML practitioners can build and deploy linear regression models in the database when their ML dataset is available in a Db2 database. In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\n",
       "Letâ€™s begin. The Regression Task\n",
       "In this exercise, I will use the GoSales dataset, which is available from this link. The dataset has 60252 synthetic customersâ€™ profile and their purchase amount at an imaginary outdoor equipment store.\n",
       "\n",
       "---\n",
       "\n",
       "**ðŸ“„ Chunk 2**\n",
       "\n",
       "In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\n",
       "Letâ€™s begin. The Regression Task\n",
       "In this exercise, I will use the GoSales dataset, which is available from this link. The dataset has 60252 synthetic customersâ€™ profile and their purchase amount at an imaginary outdoor equipment store. The following is the list of columns in the dataset:\n",
       "I want to learn a multiple linear regression function of the following equation form that will use as input the first four columns â€” AGE, MARITAL_STATUS, and PROFESSION â€” and predict the PURCHASE_AMOUNT. Using the training examples, the linear regression algorithm will learn the values of the following five parameters â€” the first one is the intercept and the remaining are four coefficients, one per input column. Following these steps, I created a Db2 table with the name GOSALES_FULL under GOSALES schema and loaded the dataset into it. Train / Test Split\n",
       "First, I will divide the records from the GOSALES_FULL table into two partitions: a training partition and a test partition. For making these partitions, I will call the SPLIT_DATA stored procedure (SP) as follows.\n",
       "\n",
       "---\n",
       "\n",
       "**ðŸ“„ Chunk 3**\n",
       "\n",
       "A.PURCHASE_AMOUNT * 100) AS MAPE FROM GOSALES.GOSALES_TEST AS A, GOSALES.GOSALES_TEST_PREDICTIONS AS B WHERE A.ID = B.ID\n",
       "Dropping the Model\n",
       "If I want to drop this model, I can use the DROP_MODEL SP:\n",
       "CALL IDAX.DROP_MODEL('model=GOSALES.GOSALES_LINREG')\n",
       "Conclusion\n",
       "In this exercise, I built, evaluated, and deployed an end-to-end linear regression pipeline using 29 simple SQL queries â€” 17 of them are calls to SPs provided by Db2, and the remaining ones were SELECT statements. In this ML workflow, I didnâ€™t need to bring any data outside of the database, nor did I need separate infrastructure for developing, training, and serving ML models. For many companies, machine learning in the database can be a cost effective and quicker path to embrace AI.\n",
       "Related Resources\n",
       "For creating an in-database ML pipeline using my above steps, you can download the GoSales dataset from this link and follow instructions on this page to set up a Db2 database. To learn more about Db2â€™s in-database ML library, check out Db2 production documentation. To find more examples of creating in-database ML models with Db2, check out this GitHub repo. #Featured-area-2\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = rag.invoke('How to build a linear regression model in Db2?')\n",
    "\n",
    "markdown_output = f\"\"\"\n",
    "## ðŸ’¡ Answer\n",
    "\n",
    "{result['result']}\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Retrieved Context\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    markdown_output += f\"\\n**ðŸ“„ Chunk {i}**\\n\\n{doc.page_content}\\n\\n---\\n\"\n",
    "\n",
    "display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bcc3e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## ðŸ’¡ Answer\n",
       "\n",
       " To evaluate a linear regression model in Db2, you can follow these steps:\n",
       "\n",
       "1. Split the data into training and testing sets using the SPLIT_DATA stored procedure (SP).\n",
       "\n",
       "CALL IDAX.SPLIT_DATA('table=GOSALES.GOSALES_FULL', 'column=ID', 'split_column=IS_TEST', 'training_set_percentage=70')\n",
       "\n",
       "2. Train the linear regression model on the training set.\n",
       "\n",
       "3. Evaluate the trained model's performance on the testing set using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n",
       "\n",
       "4. Use these evaluation metrics to make informed decisions about whether to use or improve the linear regression model in Db2.\n",
       "\n",
       "By following these steps, you can effectively evaluate a linear regression model in Db2 using SQL with a Db2 database.<|end|>\n",
       "\n",
       "---\n",
       "\n",
       "## ðŸ“š Retrieved Context\n",
       "\n",
       "\n",
       "**ðŸ“„ Chunk 1**\n",
       "\n",
       "Despite being one of the earlier machine learning techniques, linear regression continues to be a top choice among ML practitioners for a regression task. For the past three years, over 80% of the respondents to Kaggleâ€™s annual state of data science and machine learning survey mentioned linear regression as a ML algorithm they most frequently use. IBM Db2 provides an in-database stored procedure (SP) for Linear Regression as part of its ML library, which is a collection of over 200 SPs for performing different ML tasks in the database. Using the linear regression SP and other functionality of DB2â€™s ML Library, ML practitioners can build and deploy linear regression models in the database when their ML dataset is available in a Db2 database. In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\n",
       "Letâ€™s begin. The Regression Task\n",
       "In this exercise, I will use the GoSales dataset, which is available from this link. The dataset has 60252 synthetic customersâ€™ profile and their purchase amount at an imaginary outdoor equipment store.\n",
       "\n",
       "---\n",
       "\n",
       "**ðŸ“„ Chunk 2**\n",
       "\n",
       "In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\n",
       "Letâ€™s begin. The Regression Task\n",
       "In this exercise, I will use the GoSales dataset, which is available from this link. The dataset has 60252 synthetic customersâ€™ profile and their purchase amount at an imaginary outdoor equipment store. The following is the list of columns in the dataset:\n",
       "I want to learn a multiple linear regression function of the following equation form that will use as input the first four columns â€” AGE, MARITAL_STATUS, and PROFESSION â€” and predict the PURCHASE_AMOUNT. Using the training examples, the linear regression algorithm will learn the values of the following five parameters â€” the first one is the intercept and the remaining are four coefficients, one per input column. Following these steps, I created a Db2 table with the name GOSALES_FULL under GOSALES schema and loaded the dataset into it. Train / Test Split\n",
       "First, I will divide the records from the GOSALES_FULL table into two partitions: a training partition and a test partition. For making these partitions, I will call the SPLIT_DATA stored procedure (SP) as follows.\n",
       "\n",
       "---\n",
       "\n",
       "**ðŸ“„ Chunk 3**\n",
       "\n",
       "A.PURCHASE_AMOUNT * 100) AS MAPE FROM GOSALES.GOSALES_TEST AS A, GOSALES.GOSALES_TEST_PREDICTIONS AS B WHERE A.ID = B.ID\n",
       "Dropping the Model\n",
       "If I want to drop this model, I can use the DROP_MODEL SP:\n",
       "CALL IDAX.DROP_MODEL('model=GOSALES.GOSALES_LINREG')\n",
       "Conclusion\n",
       "In this exercise, I built, evaluated, and deployed an end-to-end linear regression pipeline using 29 simple SQL queries â€” 17 of them are calls to SPs provided by Db2, and the remaining ones were SELECT statements. In this ML workflow, I didnâ€™t need to bring any data outside of the database, nor did I need separate infrastructure for developing, training, and serving ML models. For many companies, machine learning in the database can be a cost effective and quicker path to embrace AI.\n",
       "Related Resources\n",
       "For creating an in-database ML pipeline using my above steps, you can download the GoSales dataset from this link and follow instructions on this page to set up a Db2 database. To learn more about Db2â€™s in-database ML library, check out Db2 production documentation. To find more examples of creating in-database ML models with Db2, check out this GitHub repo. #Featured-area-2\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = rag.invoke('How to evaluate a linear regression model in Db2?')\n",
    "\n",
    "markdown_output = f\"\"\"\n",
    "## ðŸ’¡ Answer\n",
    "\n",
    "{result['result']}\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Retrieved Context\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    markdown_output += f\"\\n**ðŸ“„ Chunk {i}**\\n\\n{doc.page_content}\\n\\n---\\n\"\n",
    "\n",
    "display(Markdown(markdown_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db2-langchain-rag-local (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
